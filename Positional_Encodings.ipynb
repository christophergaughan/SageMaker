{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOgggaNk3vBiwR+S8GO4Hag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/Positional_Encodings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model and Positional Encodings\n",
        "\n",
        "### Token to Embedding Layer\n",
        "Tokens are converted into vectors via the embedding layer:\n",
        "$$ \\text{Token} \\xrightarrow{\\text{Embedding Layer}} \\text{Embedding Vector} $$\n",
        "\n",
        "Positional encodings are added to embeddings to retain word order information:\n",
        "$$ \\text{Embedding} + \\text{Positional Encoding} = \\text{Input Representation} $$\n",
        "\n",
        "---\n",
        "\n",
        "### Transformer Components\n",
        "1. **Multi-Head Attention**:\n",
        "   Processes embeddings in parallel:\n",
        "   $[\n",
        "   \\text{Query}, \\text{Key}, \\text{Value} \\to \\text{Attention Scores} \\to \\text{Weighted Sum}\n",
        "   ]$\n",
        "\n",
        "2. **Feed-Forward Layers**:\n",
        "   Fully connected layers that process each embedding:\n",
        "   $[\n",
        "   \\text{Input} \\to \\text{Feed-Forward Neural Network} \\to \\text{Output}\n",
        "   ]$\n",
        "\n",
        "3. **Layer Normalization**:\n",
        "   Ensures stable gradients:\n",
        "   $$[\n",
        "   \\text{Output} = \\frac{\\text{Input} - \\mu}{\\sigma}\n",
        "   ]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Positional Encoding Calculation\n",
        "Positional encodings use sine and cosine functions:\n",
        "$[\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
        "]$\n",
        "$[\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
        "]$\n",
        "\n",
        "Example for \\(d = 4\\) and \\(pos = 100\\):\n",
        "$$[\n",
        "PE_{(100, 0)} = \\sin\\left(\\frac{100}{10000^{0/4}}\\right), \\quad PE_{(100, 1)} = \\cos\\left(\\frac{100}{10000^{0/4}}\\right)\n",
        "]$$\n",
        "$$[\n",
        "PE_{(100, 2)} = \\sin\\left(\\frac{100}{10000^{2/4}}\\right), \\quad PE_{(100, 3)} = \\cos\\left(\\frac{100}{10000^{2/4}}\\right)\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Key Points\n",
        "- **Positional Encodings** add information about word order to embeddings.\n",
        "- The Transformer processes all tokens in parallel.\n",
        "- Each layer normalizes, applies multi-head attention, and uses feed-forward networks to process data.\n"
      ],
      "metadata": {
        "id": "rCKkGrQ8cJtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model and Positional Encodings\n",
        "\n",
        "### Token to Embedding Layer\n",
        "Tokens are converted into vectors via the embedding layer:\n",
        "$$ \\text{Token} \\xrightarrow{\\text{Embedding Layer}} \\text{Embedding Vector} $$\n",
        "\n",
        "Positional encodings are added to embeddings to retain word order information:\n",
        "$$ \\text{Embedding} + \\text{Positional Encoding} = \\text{Input Representation} $$\n",
        "\n",
        "---\n",
        "\n",
        "### Transformer Components\n",
        "\n",
        "#### 1. **Multi-Head Attention**\n",
        "Multi-head attention calculates attention scores using:\n",
        "$$[\n",
        "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
        "]$$\n",
        "\n",
        "The scaled dot-product attention mechanism:\n",
        "$$[\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "]$$\n",
        "- $(QK^T)$: Measures similarity between queries and keys.\n",
        "- $(\\sqrt{d_k})$: Scales scores to avoid large gradients.\n",
        "- softmax: Converts scores into probabilities.\n",
        "\n",
        "Multi-head attention combines multiple heads in parallel:\n",
        "$$[\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Feed-Forward Network**\n",
        "Each embedding is processed independently through a feed-forward neural network:\n",
        "$$[\n",
        "\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Residual Connections and Layer Normalization**\n",
        "Residual connections stabilize learning by adding the input back to the processed output:\n",
        "$$[\n",
        "\\text{Output} = \\text{LayerNorm}(\\text{Input} + \\text{Processed Output})\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Positional Encoding Calculation\n",
        "\n",
        "Positional encodings use sine and cosine functions to represent position information:\n",
        "$$[\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
        "]$$\n",
        "$$[\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
        "]$$\n",
        "\n",
        "Example for \\(d = 4\\) and \\(pos = 100\\):\n",
        "$$[\n",
        "PE_{(100, 0)} = \\sin\\left(\\frac{100}{10000^{0/4}}\\right), \\quad PE_{(100, 1)} = \\cos\\left(\\frac{100}{10000^{0/4}}\\right)\n",
        "]$$\n",
        "$$[\n",
        "PE_{(100, 2)} = \\sin\\left(\\frac{100}{10000^{2/4}}\\right), \\quad PE_{(100, 3)} = \\cos\\left(\\frac{100}{10000^{2/4}}\\right)\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Why Positional Encodings Matter\n",
        "Transformers process words in parallel and lack recurrence. Positional encodings provide information about the order of words, ensuring that sequence relationships are preserved.\n",
        "\n",
        "---\n",
        "\n",
        "### Transformer Architecture Summary\n",
        "- **Input Embeddings**: Convert tokens to dense vectors.\n",
        "- **Positional Encodings**: Add positional information to embeddings.\n",
        "- **Multi-Head Attention**: Enables the model to focus on different parts of the sequence.\n",
        "- **Feed-Forward Layers**: Process embeddings independently.\n",
        "- **Residual Connections**: Stabilize learning and avoid gradient vanishing.\n",
        "- **Layer Normalization**: Normalize outputs to improve stability.\n",
        "\n"
      ],
      "metadata": {
        "id": "B4LVfHtFdumQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding \"I am a robot\" with the Transformer Model\n",
        "\n",
        "### Step 1: Tokenization\n",
        "The sentence **\"I am a robot\"** is tokenized into individual tokens:\n",
        "$$[\n",
        "\\text{Tokens: } [\"I\", \"am\", \"a\", \"robot\"]\n",
        "]$$\n",
        "\n",
        "Each token is then mapped to a unique vector through an embedding layer:\n",
        "$$[\n",
        "\\text{Embedding}(\\text{\"I\"}) = E_1, \\quad \\text{Embedding}(\\text{\"am\"}) = E_2, \\quad \\text{Embedding}(\\text{\"a\"}) = E_3, \\quad \\text{Embedding}(\\text{\"robot\"}) = E_4\n",
        "]$$\n",
        "\n",
        "Assume the embedding vectors are 4-dimensional (\\(d = 4\\)):\n",
        "$$[\n",
        "E_1 = [1.0, 0.5, 0.3, 0.2], \\quad E_2 = [0.8, 0.2, 0.5, 0.9], \\quad E_3 = [0.4, 0.7, 0.6, 0.1], \\quad E_4 = [0.9, 0.3, 0.8, 0.5]\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Positional Encoding\n",
        "Positional encodings are calculated using the following formulas:\n",
        "$$[\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right), \\quad PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
        "]$$\n",
        "Here:\n",
        "- $(pos)$ is the position of the token in the sequence (0-indexed).\n",
        "- $(d)$ is the dimensionality of the embedding (\\(d = 4\\)).\n",
        "\n",
        "#### Positional Encoding for Each Token\n",
        "- For **\"I\"** at $(pos = 0)$:\n",
        "$$[\n",
        "PE_0 = \\left[\\sin\\left(\\frac{0}{10000^{0/4}}\\right), \\cos\\left(\\frac{0}{10000^{0/4}}\\right), \\sin\\left(\\frac{0}{10000^{2/4}}\\right), \\cos\\left(\\frac{0}{10000^{2/4}}\\right)\\right] = [0, 1, 0, 1]\n",
        "]$$\n",
        "\n",
        "- For **\"am\"** at $(pos = 1)$:\n",
        "$$[\n",
        "PE_1 = \\left[\\sin\\left(\\frac{1}{10000^{0/4}}\\right), \\cos\\left(\\frac{1}{10000^{0/4}}\\right), \\sin\\left(\\frac{1}{10000^{2/4}}\\right), \\cos\\left(\\frac{1}{10000^{2/4}}\\right)\\right]\n",
        "]$$\n",
        "Approximating values:\n",
        "$$[\n",
        "PE_1 \\approx [0.8415, 0.5403, 0.001, 1.0]\n",
        "]$$\n",
        "\n",
        "- For **\"a\"** at $(pos = 2)$:\n",
        "$$[\n",
        "PE_2 = \\left[\\sin\\left(\\frac{2}{10000^{0/4}}\\right), \\cos\\left(\\frac{2}{10000^{0/4}}\\right), \\sin\\left(\\frac{2}{10000^{2/4}}\\right), \\cos\\left(\\frac{2}{10000^{2/4}}\\right)\\right]\n",
        "]$$\n",
        "Approximating values:\n",
        "$$[\n",
        "PE_2 \\approx [0.9093, -0.4161, 0.002, 1.0]\n",
        "]$$\n",
        "\n",
        "- For **\"robot\"** at \\(pos = 3\\):\n",
        "$$[\n",
        "PE_3 = \\left[\\sin\\left(\\frac{3}{10000^{0/4}}\\right), \\cos\\left(\\frac{3}{10000^{0/4}}\\right), \\sin\\left(\\frac{3}{10000^{2/4}}\\right), \\cos\\left(\\frac{3}{10000^{2/4}}\\right)\\right]\n",
        "]$$\n",
        "Approximating values:\n",
        "$$[\n",
        "PE_3 \\approx [0.1411, -0.9899, 0.003, 1.0]\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Adding Positional Encodings to Embeddings\n",
        "The final input to the Transformer is obtained by adding the positional encodings to the token embeddings:\n",
        "$$[\n",
        "\\text{Input Representation} = \\text{Embedding Vector} + \\text{Positional Encoding}\n",
        "]$$\n",
        "\n",
        "#### Combined Representations:\n",
        "- For **\"I\"**:\n",
        "$$\n",
        "\\text{Input}_1 = E_1 + PE_0 = [1.0, 0.5, 0.3, 0.2] + [0, 1, 0, 1] = [1.0, 1.5, 0.3, 1.2]\n",
        "$$\n",
        "\n",
        "- For **\"am\"**:\n",
        "$$[\n",
        "\\text{Input}_2 = E_2 + PE_1 = [0.8, 0.2, 0.5, 0.9] + [0.8415, 0.5403, 0.001, 1.0] \\approx [1.6415, 0.7403, 0.501, 1.9]\n",
        "]$$\n",
        "\n",
        "- For **\"a\"**:\n",
        "$$[\n",
        "\\text{Input}_3 = E_3 + PE_2 = [0.4, 0.7, 0.6, 0.1] + [0.9093, -0.4161, 0.002, 1.0] \\approx [1.3093, 0.2839, 0.602, 1.1]\n",
        "]$$\n",
        "\n",
        "- For **\"robot\"**:\n",
        "$$[\n",
        "\\text{Input}_4 = E_4 + PE_3 = [0.9, 0.3, 0.8, 0.5] + [0.1411, -0.9899, 0.003, 1.0] \\approx [1.0411, -0.6899, 0.803, 1.5]\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Final Encoded Representation\n",
        "The encoded input for the phrase **\"I am a robot\"** is:\n",
        "$$[\n",
        "\\text{Input} =\n",
        "\\begin{bmatrix}\n",
        "1.0 & 1.5 & 0.3 & 1.2 \\\\\n",
        "1.6415 & 0.7403 & 0.501 & 1.9 \\\\\n",
        "1.3093 & 0.2839 & 0.602 & 1.1 \\\\\n",
        "1.0411 & -0.6899 & 0.803 & 1.5 \\\\\n",
        "\\end{bmatrix}\n",
        "]$$\n",
        "\n",
        "This matrix is passed as the input to the Transformer layers.\n"
      ],
      "metadata": {
        "id": "mMGYCl9igGTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing \"I am a robot\" in the Transformer\n",
        "\n",
        "### Step 1: Input to the Transformer\n",
        "The final encoded representation of the input sentence **\"I am a robot\"** is:\n",
        "$$\n",
        "\\text{Input} =\n",
        "\\begin{bmatrix}\n",
        "1.0 & 1.5 & 0.3 & 1.2 \\\\\n",
        "1.6415 & 0.7403 & 0.501 & 1.9 \\\\\n",
        "1.3093 & 0.2839 & 0.602 & 1.1 \\\\\n",
        "1.0411 & -0.6899 & 0.803 & 1.5 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Each row corresponds to one token, and each column corresponds to one dimension of the embedding space.\n",
        "\n",
        "This matrix is passed as input to the **Transformer layers**, starting with Multi-Head Attention.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Multi-Head Attention\n",
        "Multi-head attention allows the model to focus on different parts of the sequence simultaneously. Here’s how it processes the input:\n",
        "\n",
        "#### 2.1: Compute Query (Q), Key (K), and Value (V)\n",
        "The input matrix is linearly projected into queries $((Q))$, keys $((K))$, and values $((V))$ using learned weight matrices:\n",
        "$$[\n",
        "Q = \\text{Input} \\cdot W^Q, \\quad K = \\text{Input} \\cdot W^K, \\quad V = \\text{Input} \\cdot W^V\n",
        "]$$\n",
        "Assume $(W^Q)$, $(W^K)$, and $(W^V)$ are $(4 \\times 4)$ matrices (same as embedding dimension). After projection, we get:\n",
        "$$[\n",
        "Q, K, V \\in \\mathbb{R}^{4 \\times 4}\n",
        "]$$\n",
        "\n",
        "#### 2.2: Compute Attention Scores\n",
        "Attention scores are computed by taking the dot product of $(Q)$ and $(K^T)$, then scaling by the square root of the embedding dimension $((\\sqrt{d_k}))$:\n",
        "$$[\n",
        "\\text{Attention Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
        "]$$\n",
        "Assuming $(d_k = 4)$, scaling factor = $(2)$.\n",
        "\n",
        "#### 2.3: Apply Softmax\n",
        "The attention scores are passed through a softmax function to normalize them into probabilities:\n",
        "$$\n",
        "\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
        "$$\n",
        "\n",
        "#### 2.4: Weighted Sum of Values\n",
        "The attention probabilities are used to compute a weighted sum of the value vectors $((V))$:\n",
        "$$\n",
        "\\text{Output from Attention} = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "This operation creates a new set of vectors that combine information from all tokens.\n",
        "\n",
        "#### 2.5: Multi-Head Attention\n",
        "Multiple attention heads compute this process independently, and their results are concatenated:\n",
        "$$[\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O\n",
        "]$$\n",
        "\n",
        "Here $(W^O)$ is another learned weight matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Residual Connection and Layer Normalization\n",
        "The output of the multi-head attention layer is added back to the original input (residual connection):\n",
        "$$[\n",
        "\\text{Residual Output} = \\text{Input} + \\text{Attention Output}\n",
        "]$$\n",
        "\n",
        "Then, layer normalization is applied to stabilize training:\n",
        "$$[\n",
        "\\text{Normalized Output} = \\frac{\\text{Residual Output} - \\mu}{\\sigma}\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Feed-Forward Network\n",
        "The normalized output is passed through a position-wise feed-forward network:\n",
        "$$[\n",
        "\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
        "]$$\n",
        "This network applies two linear transformations with a ReLU activation function in between. Each token is processed independently.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Repeated Transformer Layers\n",
        "The Transformer layer (Multi-Head Attention + Feed-Forward Network) is repeated $(N)$ times (e.g., 6 layers in the original Transformer model). Each layer refines the token representations further, allowing the model to build richer contextual embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 6: Output from Encoder\n",
        "After passing through all Transformer layers, the encoded representation for **\"I am a robot\"** is ready. This encoded representation contains contextual information about the entire sentence, allowing the model to understand the relationships between tokens.\n",
        "\n",
        "$$[\n",
        "\\text{Final Encoded Representation (Encoder Output)} =\n",
        "\\begin{bmatrix}\n",
        "h_1 \\\\\n",
        "h_2 \\\\\n",
        "h_3 \\\\\n",
        "h_4 \\\\\n",
        "\\end{bmatrix}\n",
        "]$$\n",
        "Each $(h_i)$ is a refined vector representation for the corresponding token.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 7: Decoder (Optional, for Sequence-to-Sequence Models)\n",
        "If this model is part of a sequence-to-sequence architecture (e.g., machine translation), the encoded representation is passed to the decoder. The decoder uses this representation, along with its own self-attention mechanism, to generate the output sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### Recap of the Process for \"I am a robot\"\n",
        "1. **Input Representation**: Combine embeddings and positional encodings.\n",
        "2. **Multi-Head Attention**: Learn relationships between tokens.\n",
        "3. **Residual and Normalization**: Add stability and prevent vanishing gradients.\n",
        "4. **Feed-Forward Network**: Process token embeddings independently.\n",
        "5. **Repeat**: Stack multiple layers to refine representations.\n",
        "6. **Output**: Encoded sentence ready for downstream tasks (e.g., classification, translation).\n",
        "\n"
      ],
      "metadata": {
        "id": "gVelR25fjJaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkfzG2mLb_vy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Activate seaborn's style for better visuals\n",
        "sns.set_theme()\n",
        "\n",
        "# Define the ReLU function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Generate x values for the graph\n",
        "x = np.linspace(-10, 10, 500)  # 500 points between -10 and 10\n",
        "\n",
        "# Compute ReLU values for x\n",
        "y = relu(x)\n",
        "\n",
        "# Plot the ReLU function\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size\n",
        "plt.plot(x, y, label='ReLU(x)', color='blue', linewidth=2)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('ReLU (Rectified Linear Unit) Function', fontsize=16)\n",
        "plt.xlabel('x', fontsize=14)\n",
        "plt.ylabel('ReLU(x)', fontsize=14)\n",
        "\n",
        "# Add a grid for better readability\n",
        "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(fontsize=12, loc='upper left')\n",
        "\n",
        "# Show the graph\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Activate seaborn's style for better visuals\n",
        "sns.set_theme()\n",
        "\n",
        "# Define the Softmax function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # Subtract max(x) for numerical stability\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# Generate multiple input categories\n",
        "x_values = np.linspace(-5, 5, 100)  # 100 points between -5 and 5\n",
        "categories = np.array([x_values, x_values + 2, x_values - 2, x_values + 1])  # 4 \"categories\" for Softmax\n",
        "\n",
        "# Compute the Softmax values for these categories\n",
        "softmax_outputs = np.apply_along_axis(softmax, axis=0, arr=categories)\n",
        "\n",
        "# Plot the Softmax function for all categories\n",
        "plt.figure(figsize=(10, 7))  # Set the figure size\n",
        "for i, softmax_curve in enumerate(softmax_outputs):\n",
        "    plt.plot(x_values, softmax_curve, label=f'Category {i+1}', linewidth=2)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Softmax Function Shape Across Categories', fontsize=16)\n",
        "plt.xlabel('Input Value (x)', fontsize=14)\n",
        "plt.ylabel('Softmax Probability', fontsize=14)\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(fontsize=12, loc='upper right')\n",
        "\n",
        "# Show the graph\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j7sKhFh0m9uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Activate seaborn's style for better visuals\n",
        "sns.set_theme()\n",
        "\n",
        "# Define the Softmax function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # Subtract max(x) for numerical stability\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# Generate input values for one dimension and some fixed competitors\n",
        "x = np.linspace(-10, 10, 500)  # 500 points between -10 and 10\n",
        "fixed_competitors = [-2, 0, 2]  # Fixed inputs for other \"categories\"\n",
        "\n",
        "# Create inputs for a single class (varies) and competitors (fixed)\n",
        "inputs = np.vstack([x] + [np.full_like(x, c) for c in fixed_competitors])\n",
        "\n",
        "# Compute Softmax probabilities\n",
        "softmax_outputs = np.apply_along_axis(softmax, axis=0, arr=inputs)\n",
        "\n",
        "# Plot the Softmax function for the varying category\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, softmax_outputs[0], label='Softmax of Varying Input', color='blue', linewidth=2)\n",
        "\n",
        "# Add horizontal line for comparison (other classes)\n",
        "for i, prob in enumerate(softmax_outputs[1:], start=1):\n",
        "    plt.axhline(y=prob.mean(), color='gray', linestyle='--', label=f'Fixed Competitor {i}')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Sigmoid Shape of Softmax Function', fontsize=16)\n",
        "plt.xlabel('Input Value (x)', fontsize=14)\n",
        "plt.ylabel('Softmax Probability', fontsize=14)\n",
        "\n",
        "# Add a grid for better readability\n",
        "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(fontsize=12, loc='center right')\n",
        "\n",
        "# Show the graph\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XLko4GhxnW6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZvY_wZ-nx6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}