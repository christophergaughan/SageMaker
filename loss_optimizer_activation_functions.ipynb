{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:42.726716Z",
     "start_time": "2024-02-04T00:08:42.723323Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data_optimizers = {\n",
    "    \"Optimizer\": [\"SGD\", \"Adam\", \"RMSprop\", \"Adagrad\", \"Adadelta\"],\n",
    "    \"Description\": [\n",
    "        \"A simple yet effective optimizer. It updates the model's weights based on the gradient of the loss function with respect to the weight.\",\n",
    "        \"Combines ideas from RMSProp and SGD with momentum. It computes adaptive learning rates for each parameter.\",\n",
    "        \"This optimizer adjusts the learning rate for each weight based on the recent magnitudes of the gradients for that weight.\",\n",
    "        \"Adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent ones.\",\n",
    "        \"An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\"\n",
    "    ],\n",
    "    \"Usage\": [\n",
    "        \"Good for a wide range of problems but may require tuning of the learning rate and can be slow.\",\n",
    "        \"Excellent for large datasets and high-dimensional parameter spaces.\",\n",
    "        \"Effective for recurrent neural networks and other contexts where the gradient may change direction quickly.\",\n",
    "        \"Good for sparse data (e.g., text data, recommender systems).\",\n",
    "        \"Useful in situations requiring finer control over learning rates.\"\n",
    "    ],\n",
    "    \"Why Used\": [\n",
    "        \"It's the foundational method for neural network training. Variants with momentum are used to accelerate convergence.\",\n",
    "        \"Often provides faster convergence than SGD and requires less fine-tuning of the learning rate.\",\n",
    "        \"Helps resolve issues like vanishing or exploding gradients in SGD.\",\n",
    "        \"Automatically adjusts the learning rate, reducing the need for manual tuning.\",\n",
    "        \"Addresses the diminishing learning rates problem of Adagrad.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_optimizers = pd.DataFrame(data_optimizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62ed43bf5e6ab6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:44.062594Z",
     "start_time": "2024-02-04T00:08:44.053390Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_styled = df_optimizers.style.set_properties(**{'text-align': 'left', 'white-space': 'normal'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5c9ecec6014db1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:44.826665Z",
     "start_time": "2024-02-04T00:08:44.820221Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_088c8_row0_col0, #T_088c8_row0_col1, #T_088c8_row0_col2, #T_088c8_row0_col3, #T_088c8_row1_col0, #T_088c8_row1_col1, #T_088c8_row1_col2, #T_088c8_row1_col3, #T_088c8_row2_col0, #T_088c8_row2_col1, #T_088c8_row2_col2, #T_088c8_row2_col3, #T_088c8_row3_col0, #T_088c8_row3_col1, #T_088c8_row3_col2, #T_088c8_row3_col3, #T_088c8_row4_col0, #T_088c8_row4_col1, #T_088c8_row4_col2, #T_088c8_row4_col3 {\n",
       "  text-align: left;\n",
       "  white-space: normal;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_088c8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_088c8_level0_col0\" class=\"col_heading level0 col0\" >Optimizer</th>\n",
       "      <th id=\"T_088c8_level0_col1\" class=\"col_heading level0 col1\" >Description</th>\n",
       "      <th id=\"T_088c8_level0_col2\" class=\"col_heading level0 col2\" >Usage</th>\n",
       "      <th id=\"T_088c8_level0_col3\" class=\"col_heading level0 col3\" >Why Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_088c8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_088c8_row0_col0\" class=\"data row0 col0\" >SGD</td>\n",
       "      <td id=\"T_088c8_row0_col1\" class=\"data row0 col1\" >A simple yet effective optimizer. It updates the model's weights based on the gradient of the loss function with respect to the weight.</td>\n",
       "      <td id=\"T_088c8_row0_col2\" class=\"data row0 col2\" >Good for a wide range of problems but may require tuning of the learning rate and can be slow.</td>\n",
       "      <td id=\"T_088c8_row0_col3\" class=\"data row0 col3\" >It's the foundational method for neural network training. Variants with momentum are used to accelerate convergence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_088c8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_088c8_row1_col0\" class=\"data row1 col0\" >Adam</td>\n",
       "      <td id=\"T_088c8_row1_col1\" class=\"data row1 col1\" >Combines ideas from RMSProp and SGD with momentum. It computes adaptive learning rates for each parameter.</td>\n",
       "      <td id=\"T_088c8_row1_col2\" class=\"data row1 col2\" >Excellent for large datasets and high-dimensional parameter spaces.</td>\n",
       "      <td id=\"T_088c8_row1_col3\" class=\"data row1 col3\" >Often provides faster convergence than SGD and requires less fine-tuning of the learning rate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_088c8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_088c8_row2_col0\" class=\"data row2 col0\" >RMSprop</td>\n",
       "      <td id=\"T_088c8_row2_col1\" class=\"data row2 col1\" >This optimizer adjusts the learning rate for each weight based on the recent magnitudes of the gradients for that weight.</td>\n",
       "      <td id=\"T_088c8_row2_col2\" class=\"data row2 col2\" >Effective for recurrent neural networks and other contexts where the gradient may change direction quickly.</td>\n",
       "      <td id=\"T_088c8_row2_col3\" class=\"data row2 col3\" >Helps resolve issues like vanishing or exploding gradients in SGD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_088c8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_088c8_row3_col0\" class=\"data row3 col0\" >Adagrad</td>\n",
       "      <td id=\"T_088c8_row3_col1\" class=\"data row3 col1\" >Adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent ones.</td>\n",
       "      <td id=\"T_088c8_row3_col2\" class=\"data row3 col2\" >Good for sparse data (e.g., text data, recommender systems).</td>\n",
       "      <td id=\"T_088c8_row3_col3\" class=\"data row3 col3\" >Automatically adjusts the learning rate, reducing the need for manual tuning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_088c8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_088c8_row4_col0\" class=\"data row4 col0\" >Adadelta</td>\n",
       "      <td id=\"T_088c8_row4_col1\" class=\"data row4 col1\" >An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.</td>\n",
       "      <td id=\"T_088c8_row4_col2\" class=\"data row4 col2\" >Useful in situations requiring finer control over learning rates.</td>\n",
       "      <td id=\"T_088c8_row4_col3\" class=\"data row4 col3\" >Addresses the diminishing learning rates problem of Adagrad.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x142459910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Assuming df_optimizers is already created\n",
    "\n",
    "df_styled = df_optimizers.style.set_properties(**{'text-align': 'left', 'white-space': 'normal'})\n",
    "display(df_styled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "356e4a87429efb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:46.284378Z",
     "start_time": "2024-02-04T00:08:46.279402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d82d2e4efa6b08",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "343b74109271844b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mathematical Differences Between Weights and Biases in Neural Networks\n",
    "\n",
    "In neural networks, weights and biases are fundamental parameters that influence how the network processes input data and makes predictions. While they are both crucial to the network's ability to learn, they serve different mathematical roles.\n",
    "\n",
    "## Weights in Neural Networks\n",
    "\n",
    "Weights determine the strength of the influence of one neuron on another. They are applied to input data and adjusted during the training process.\n",
    "\n",
    "### Mathematical Representation:\n",
    "- Consider a neural network with inputs $x_1, x_2, \\ldots, x_n$ and corresponding weights $w_1, w_2, \\ldots, w_n$. For a single neuron, the weighted sum of its inputs is given by:\n",
    "  \n",
    "  $$\n",
    "  z = w_1x_1 + w_2x_2 + \\ldots + w_nx_n\n",
    "  $$\n",
    "\n",
    "- Weights scale the input data and are key to the network's ability to represent complex relationships between inputs and outputs.\n",
    "\n",
    "## Biases in Neural Networks\n",
    "\n",
    "A bias is an additional parameter that allows the neural network to adjust its output independently of its weighted input.\n",
    "\n",
    "### Mathematical Representation:\n",
    "- The bias term is added to the weighted sum before the activation function is applied. If the bias for a neuron is represented as \\( b \\), the output of the neuron before activation is:\n",
    "\n",
    "  $$\n",
    "  z = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b\n",
    "  $$\n",
    "\n",
    "- The bias shifts the activation function, allowing the neuron to represent patterns that do not necessarily pass through the origin.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Weights** (\\( w_i \\)) scale the input signals, influencing the network's internal representation of the input.\n",
    "- **Biases** (\\( b \\)) provide a way to shift the activation function to better fit the data, offering an additional degree of freedom.\n",
    "\n",
    "Together, weights and biases enable a neural network to learn complex patterns and make accurate predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer in Deep Neural Networks\n",
    "\n",
    "In a deep neural network, the **optimizer** is a critical component that influences how the network learns from its training data. The primary function of an optimizer is to adjust the network's weights and biases to minimize the loss function. This process is crucial in learning the mapping from inputs to outputs that the network is trying to capture.\n",
    "\n",
    "## Function of the Optimizer\n",
    "\n",
    "The optimizer takes the gradients of the loss function with respect to the network's parameters (weights and biases) and updates these parameters in a direction that minimizes the loss. This is generally done using some form of gradient descent. The basic update rule for a parameter $w$ using gradient descent is:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\nabla_w J(w)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate, and $\\nabla_w J(w)$ is the gradient of the loss function $J(w)$ with respect to the parameter $w$.\n",
    "\n",
    "## Where It Is Found in the Network\n",
    "\n",
    "The optimizer operates across the entire network, affecting all layers. However, its influence is not uniform:\n",
    "\n",
    "- **Input Layer**: While the input layer receives the raw data, the optimizer does not directly affect this layer, as it has no weights or biases to adjust.\n",
    "- **Deep Layers (Hidden Layers)**: The optimizer plays a significant role here, adjusting weights and biases based on backpropagated gradients. These layers are where the majority of learning and feature extraction happens.\n",
    "- **Output Layer**: In the output layer, the optimizer fine-tunes the weights and biases to ensure the final predictions are as close as possible to the actual values or labels.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The choice of optimizer and its settings (like the learning rate) can significantly impact the efficiency and effectiveness of a neural network's training process. Common optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, etc., each with its own strengths and ideal use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89a9d8be4711694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:51.078238Z",
     "start_time": "2024-02-04T00:08:51.074858Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # or use a specific width like 100\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6783bfffa35a11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:53.588726Z",
     "start_time": "2024-02-04T00:08:53.583771Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Description</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Why Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD</td>\n",
       "      <td>A simple yet effective optimizer. It updates the model's weights based on the gradient of the loss function with respect to the weight.</td>\n",
       "      <td>Good for a wide range of problems but may require tuning of the learning rate and can be slow.</td>\n",
       "      <td>It's the foundational method for neural network training. Variants with momentum are used to accelerate convergence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam</td>\n",
       "      <td>Combines ideas from RMSProp and SGD with momentum. It computes adaptive learning rates for each parameter.</td>\n",
       "      <td>Excellent for large datasets and high-dimensional parameter spaces.</td>\n",
       "      <td>Often provides faster convergence than SGD and requires less fine-tuning of the learning rate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RMSprop</td>\n",
       "      <td>This optimizer adjusts the learning rate for each weight based on the recent magnitudes of the gradients for that weight.</td>\n",
       "      <td>Effective for recurrent neural networks and other contexts where the gradient may change direction quickly.</td>\n",
       "      <td>Helps resolve issues like vanishing or exploding gradients in SGD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adagrad</td>\n",
       "      <td>Adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent ones.</td>\n",
       "      <td>Good for sparse data (e.g., text data, recommender systems).</td>\n",
       "      <td>Automatically adjusts the learning rate, reducing the need for manual tuning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adadelta</td>\n",
       "      <td>An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.</td>\n",
       "      <td>Useful in situations requiring finer control over learning rates.</td>\n",
       "      <td>Addresses the diminishing learning rates problem of Adagrad.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Optimizer  \\\n",
       "0       SGD   \n",
       "1      Adam   \n",
       "2   RMSprop   \n",
       "3   Adagrad   \n",
       "4  Adadelta   \n",
       "\n",
       "                                                                                                                               Description  \\\n",
       "0  A simple yet effective optimizer. It updates the model's weights based on the gradient of the loss function with respect to the weight.   \n",
       "1                               Combines ideas from RMSProp and SGD with momentum. It computes adaptive learning rates for each parameter.   \n",
       "2                This optimizer adjusts the learning rate for each weight based on the recent magnitudes of the gradients for that weight.   \n",
       "3  Adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent ones.   \n",
       "4                                     An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.   \n",
       "\n",
       "                                                                                                         Usage  \\\n",
       "0               Good for a wide range of problems but may require tuning of the learning rate and can be slow.   \n",
       "1                                          Excellent for large datasets and high-dimensional parameter spaces.   \n",
       "2  Effective for recurrent neural networks and other contexts where the gradient may change direction quickly.   \n",
       "3                                                 Good for sparse data (e.g., text data, recommender systems).   \n",
       "4                                            Useful in situations requiring finer control over learning rates.   \n",
       "\n",
       "                                                                                                               Why Used  \n",
       "0  It's the foundational method for neural network training. Variants with momentum are used to accelerate convergence.  \n",
       "1                        Often provides faster convergence than SGD and requires less fine-tuning of the learning rate.  \n",
       "2                                                    Helps resolve issues like vanishing or exploding gradients in SGD.  \n",
       "3                                         Automatically adjusts the learning rate, reducing the need for manual tuning.  \n",
       "4                                                          Addresses the diminishing learning rates problem of Adagrad.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_optimizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "313065bbf40d5cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:08:55.437596Z",
     "start_time": "2024-02-04T00:08:55.430851Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea9ec7003c9f39d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Purpose of Loss Functions in Deep Learning\n",
    "\n",
    "In deep learning, **loss functions** play a crucial role in guiding the training process of models. These functions are used to measure the difference between the model's predictions and the actual target values. The main goal of a loss function is to minimize this difference, thereby improving the model's accuracy.\n",
    "\n",
    "## General Purpose of Loss Functions\n",
    "\n",
    "- **Measuring Error**: Loss functions quantify the error between predicted values and actual values. This error measurement is critical for model training.\n",
    "- **Guiding Model Training**: By minimizing the loss, the model learns to make predictions that are as close as possible to the true values. The training process involves adjusting model parameters (weights and biases) to reduce this loss.\n",
    "\n",
    "## Where Loss Functions Are Used in Deep Learning\n",
    "\n",
    "### In Frameworks like Keras/TensorFlow\n",
    "\n",
    "- **End of the Network**: In deep learning frameworks such as Keras and TensorFlow, the loss function is typically defined at the compiling stage of the model and is applied at the output layer.\n",
    "- **Backpropagation**: During training, after forward propagation, the loss is computed at the output layer. This loss is then used in backpropagation to update the model's weights, where gradients of the loss function are calculated with respect to each weight.\n",
    "- **Optimization**: The choice of loss function is closely tied to the optimizer used in the model. The optimizer uses the gradients of the loss function to adjust the weights.\n",
    "\n",
    "## Types of Loss Functions\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Commonly used in regression tasks.\n",
    "- **Categorical/Binary Crossentropy**: Used in classification tasks (multi-class and binary).\n",
    "- **Hinge Loss**: Often used in \"maximum-margin\" classification, like in Support Vector Machines (SVMs).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Choosing the appropriate loss function is crucial in deep learning models. It depends on the specific type of problem being solved (e.g., regression vs. classification) and can significantly impact the performance and effectiveness of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bdf80b925456ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:12:28.460066Z",
     "start_time": "2024-02-04T00:12:28.456807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8746e8b6baebde32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:12:41.774729Z",
     "start_time": "2024-02-04T00:12:41.764619Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Why Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean Squared Error (MSE)</td>\n",
       "      <td>Regression problems.</td>\n",
       "      <td>Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification problems.</td>\n",
       "      <td>Measures the difference between two probability distributions - the actual labels and the predicted labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Binary Crossentropy</td>\n",
       "      <td>Binary classification problems.</td>\n",
       "      <td>Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparse Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification tasks with many classes.</td>\n",
       "      <td>Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hinge Loss</td>\n",
       "      <td>\"Maximum-margin\" classification, mostly used for Support Vector Machines (SVMs).</td>\n",
       "      <td>Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Loss Function  \\\n",
       "0         Mean Squared Error (MSE)   \n",
       "1         Categorical Crossentropy   \n",
       "2              Binary Crossentropy   \n",
       "3  Sparse Categorical Crossentropy   \n",
       "4                       Hinge Loss   \n",
       "\n",
       "                                                                              Usage  \\\n",
       "0                                                              Regression problems.   \n",
       "1                                              Multi-class classification problems.   \n",
       "2                                                   Binary classification problems.   \n",
       "3                               Multi-class classification tasks with many classes.   \n",
       "4  \"Maximum-margin\" classification, mostly used for Support Vector Machines (SVMs).   \n",
       "\n",
       "                                                                                                                                        Why Used  \n",
       "0         Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.  \n",
       "1                                    Measures the difference between two probability distributions - the actual labels and the predicted labels.  \n",
       "2    Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.  \n",
       "3  Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).  \n",
       "4                Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust max width of the column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Data for the DataFrame\n",
    "data_loss_functions = {\n",
    "    \"Loss Function\": [\"Mean Squared Error (MSE)\", \"Categorical Crossentropy\", \"Binary Crossentropy\", \"Sparse Categorical Crossentropy\", \"Hinge Loss\"],\n",
    "    \"Usage\": [\n",
    "        \"Regression problems.\",\n",
    "        \"Multi-class classification problems.\",\n",
    "        \"Binary classification problems.\",\n",
    "        \"Multi-class classification tasks with many classes.\",\n",
    "        \"\\\"Maximum-margin\\\" classification, mostly used for Support Vector Machines (SVMs).\"\n",
    "    ],\n",
    "    \"Why Used\": [\n",
    "        \"Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.\",\n",
    "        \"Measures the difference between two probability distributions - the actual labels and the predicted labels.\",\n",
    "        \"Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.\",\n",
    "        \"Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).\",\n",
    "        \"Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_loss_functions = pd.DataFrame(data_loss_functions)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df_loss_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cd588ca1232373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:26:27.165551Z",
     "start_time": "2024-02-04T00:26:27.161906Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9cc8849c26bd4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:30:03.671897Z",
     "start_time": "2024-02-04T00:30:03.651785Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    table, th, td {\n",
       "      border: 1px solid black;\n",
       "      border-collapse: collapse;\n",
       "    }\n",
       "    th, td {\n",
       "      padding: 10px;\n",
       "      text-align: left;\n",
       "      max-width: 150px; /* Adjust based on your requirement */\n",
       "      word-wrap: break-word;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Why Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean Squared Error (MSE)</td>\n",
       "      <td>Regression problems.</td>\n",
       "      <td>Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification problems.</td>\n",
       "      <td>Measures the difference between two probability distributions - the actual labels and the predicted labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Binary Crossentropy</td>\n",
       "      <td>Binary classification problems.</td>\n",
       "      <td>Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparse Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification tasks with many classes.</td>\n",
       "      <td>Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hinge Loss</td>\n",
       "      <td>\"Maximum-margin\" classification, mostly used for Support Vector Machines (SVMs).</td>\n",
       "      <td>Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Your DataFrame\n",
    "data_loss_functions = {\n",
    "    \"Loss Function\": [\"Mean Squared Error (MSE)\", \"Categorical Crossentropy\", \"Binary Crossentropy\", \"Sparse Categorical Crossentropy\", \"Hinge Loss\"],\n",
    "    \"Usage\": [\n",
    "        \"Regression problems.\",\n",
    "        \"Multi-class classification problems.\",\n",
    "        \"Binary classification problems.\",\n",
    "        \"Multi-class classification tasks with many classes.\",\n",
    "        \"\\\"Maximum-margin\\\" classification, mostly used for Support Vector Machines (SVMs).\"\n",
    "    ],\n",
    "    \"Why Used\": [\n",
    "        \"Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.\",\n",
    "        \"Measures the difference between two probability distributions - the actual labels and the predicted labels.\",\n",
    "        \"Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.\",\n",
    "        \"Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).\",\n",
    "        \"Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_loss_functions = pd.DataFrame(data_loss_functions)\n",
    "\n",
    "# Convert DataFrame to HTML\n",
    "html = df_loss_functions.to_html(escape=False)\n",
    "\n",
    "# Add CSS to wrap text within table cells\n",
    "html_style = \"\"\"\n",
    "<style>\n",
    "    table, th, td {\n",
    "      border: 1px solid black;\n",
    "      border-collapse: collapse;\n",
    "    }\n",
    "    th, td {\n",
    "      padding: 10px;\n",
    "      text-align: left;\n",
    "      max-width: 150px; /* Adjust based on your requirement */\n",
    "      word-wrap: break-word;\n",
    "    }\n",
    "</style>\n",
    "\"\"\" + html\n",
    "\n",
    "# Display HTML with style\n",
    "display(HTML(html_style))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80419f3d9e61b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:15:28.983395Z",
     "start_time": "2024-02-04T00:15:28.936603Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Loss Function</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Why Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Mean Squared Error (MSE)</td>\n",
       "      <td>Regression problems.</td>\n",
       "      <td>Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification problems.</td>\n",
       "      <td>Measures the difference between two probability distributions - the actual labels and the predicted labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Binary Crossentropy</td>\n",
       "      <td>Binary classification problems.</td>\n",
       "      <td>Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sparse Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification tasks with many classes.</td>\n",
       "      <td>Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hinge Loss</td>\n",
       "      <td>\"Maximum-margin\" classification, most used for Support Vector Machines (SVMs).</td>\n",
       "      <td>Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    table.dataframe td, table.dataframe th {\n",
       "        max-width: 300px;\n",
       "        white-space: nowrap;\n",
       "        overflow: hidden;\n",
       "        text-overflow: ellipsis;\n",
       "    }\n",
       "    table.dataframe td {\n",
       "        min-width: 100px;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Style\n",
    "style = \"\"\"\n",
    "<style>\n",
    "    table.dataframe td, table.dataframe th {\n",
    "        max-width: 300px;\n",
    "        white-space: nowrap;\n",
    "        overflow: hidden;\n",
    "        text-overflow: ellipsis;\n",
    "    }\n",
    "    table.dataframe td {\n",
    "        min-width: 100px;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Data for the DataFrame\n",
    "data_loss_functions = {\n",
    "    \"Loss Function\": [\"Mean Squared Error (MSE)\", \"Categorical Crossentropy\", \"Binary Crossentropy\", \"Sparse Categorical Crossentropy\", \"Hinge Loss\"],\n",
    "    \"Usage\": [\n",
    "        \"Regression problems.\",\n",
    "        \"Multi-class classification problems.\",\n",
    "        \"Binary classification problems.\",\n",
    "        \"Multi-class classification tasks with many classes.\",\n",
    "        \"\\\"Maximum-margin\\\" classification, most used for Support Vector Machines (SVMs).\"\n",
    "    ],\n",
    "    \"Why Used\": [\n",
    "        \"Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.\",\n",
    "        \"Measures the difference between two probability distributions - the actual labels and the predicted labels.\",\n",
    "        \"Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.\",\n",
    "        \"Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).\",\n",
    "        \"Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_loss_functions = pd.DataFrame(data_loss_functions)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(HTML(df_loss_functions.to_html(index=False)))\n",
    "\n",
    "# Apply the CSS\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba0a7a13f212603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:16:45.285647Z",
     "start_time": "2024-02-04T00:16:45.273128Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Why Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean Squared Error (MSE)</td>\n",
       "      <td>Regression problems.</td>\n",
       "      <td>Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification problems.</td>\n",
       "      <td>Measures the difference between two probability distributions - the actual labels and the predicted labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Binary Crossentropy</td>\n",
       "      <td>Binary classification problems.</td>\n",
       "      <td>Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparse Categorical Crossentropy</td>\n",
       "      <td>Multi-class classification tasks with many classes.</td>\n",
       "      <td>Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hinge Loss</td>\n",
       "      <td>\"Maximum-margin\" classification, mostly used for Support Vector Machines (SVMs).</td>\n",
       "      <td>Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Loss Function  \\\n",
       "0         Mean Squared Error (MSE)   \n",
       "1         Categorical Crossentropy   \n",
       "2              Binary Crossentropy   \n",
       "3  Sparse Categorical Crossentropy   \n",
       "4                       Hinge Loss   \n",
       "\n",
       "                                                                              Usage  \\\n",
       "0                                                              Regression problems.   \n",
       "1                                              Multi-class classification problems.   \n",
       "2                                                   Binary classification problems.   \n",
       "3                               Multi-class classification tasks with many classes.   \n",
       "4  \"Maximum-margin\" classification, mostly used for Support Vector Machines (SVMs).   \n",
       "\n",
       "                                                                                                                                        Why Used  \n",
       "0         Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.  \n",
       "1                                    Measures the difference between two probability distributions - the actual labels and the predicted labels.  \n",
       "2    Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.  \n",
       "3  Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).  \n",
       "4                Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data_loss_functions = {\n",
    "    \"Loss Function\": [\"Mean Squared Error (MSE)\", \"Categorical Crossentropy\", \"Binary Crossentropy\", \"Sparse Categorical Crossentropy\", \"Hinge Loss\"],\n",
    "    \"Usage\": [\n",
    "        \"Regression problems.\",\n",
    "        \"Multi-class classification problems.\",\n",
    "        \"Binary classification problems.\",\n",
    "        \"Multi-class classification tasks with many classes.\",\n",
    "        \"\\\"Maximum-margin\\\" classification, mostly used for Support Vector Machines (SVMs).\"\n",
    "    ],\n",
    "    \"Why Used\": [\n",
    "        \"Measures the average of the squares of the errors between actual and predicted values. Good for ensuring small errors are not ignored.\",\n",
    "        \"Measures the difference between two probability distributions - the actual labels and the predicted labels.\",\n",
    "        \"Special case of categorical crossentropy for two-class problems. Suitable for measuring the error in classification tasks with two classes.\",\n",
    "        \"Useful when the classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array with a single non-zero element).\",\n",
    "        \"Encourages the model to correctly classify data while maintaining a large margin between data points and the decision boundary.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_loss_functions = pd.DataFrame(data_loss_functions)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df_loss_functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1886869bcb93e10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Purpose of Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions in neural networks are crucial for introducing non-linearity into the model. They enable neural networks to capture complex relationships in data and perform tasks that go beyond mere linear mappings.\n",
    "\n",
    "## Role of Activation Functions\n",
    "\n",
    "- **Introducing Non-Linearity**: Without non-linear activation functions, a neural network, regardless of its depth, would behave like a single-layer linear model. Non-linearity allows the network to approximate complex functions.\n",
    "  \n",
    "- **Enabling Complex Representations**: By applying activation functions, neural networks can represent complex functions and solve various types of problems like classification, regression, and more.\n",
    "\n",
    "## Mathematical Essence\n",
    "\n",
    "- The choice of activation function affects how the inputs are transformed to outputs within a neuron. For example, a common activation function is the Rectified Linear Unit (ReLU), defined as:\n",
    "\n",
    "  $$\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  $$\n",
    "\n",
    "- Another example is the hyperbolic tangent function (tanh), which outputs values between -1 and 1:\n",
    "\n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "  $$\n",
    "\n",
    "## Usage in Neural Networks\n",
    "\n",
    "- **Hidden Layers**: Activation functions like ReLU and tanh are commonly used in hidden layers. They allow the network to learn and represent complex patterns.\n",
    "\n",
    "- **Output Layer**: For classification tasks, functions like softmax are used in the output layer to interpret the neural network outputs as probabilities. The softmax function is defined as:\n",
    "\n",
    "  $$\n",
    "  \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "  $$\n",
    "\n",
    "  where \\(x_i\\) is the input to the output neuron \\(i\\), and the denominator is the sum of the exponentials of all inputs to the output layer.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Activation functions are a fundamental aspect of neural networks, enabling them to learn and make sense of complex, non-linear relationships in data. The choice of activation function depends on the specific requirements of the neural network architecture and the task at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b84ccf93757b139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:16:46.603852Z",
     "start_time": "2024-02-04T00:16:46.591516Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Characteristics</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Benefits</th>\n",
       "      <th>Drawbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReLU</td>\n",
       "      <td>ReLU(x) = max(0, x)</td>\n",
       "      <td>Piecewise linear function that outputs the input directly if it's positive, otherwise outputs zero.</td>\n",
       "      <td>Widely used in hidden layers of neural networks.</td>\n",
       "      <td>Helps alleviate the vanishing gradient problem. Computationally efficient and allows for quick convergence.</td>\n",
       "      <td>ReLU units can be fragile during training and can 'die' if large gradients flow through them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh(x) = (e^x - e^-x) / (e^x + e^-x)</td>\n",
       "      <td>Scaled and shifted version of the sigmoid function. Its output ranges from -1 to 1.</td>\n",
       "      <td>Commonly used in hidden layers, though less frequent than ReLU in modern networks.</td>\n",
       "      <td>Can model complex relationships due to its non-linear shape. Useful when data needs to be normalized around zero.</td>\n",
       "      <td>Similar to sigmoid, it suffers from the vanishing gradient problem, which can make it less effective in deep networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Softmax</td>\n",
       "      <td>Softmax(x) = e^x_i / sum(e^x) for i in x</td>\n",
       "      <td>Converts a vector of values into a probability distribution.</td>\n",
       "      <td>Typically used in the output layer of a neural network for multiclass classification tasks.</td>\n",
       "      <td>Provides a clear probabilistic framework for multiclass classification. Each output class's probability sums to 1.</td>\n",
       "      <td>Not suitable for non-classification tasks. The exponentiation can cause numerical stability issues.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Function                                   Formula  \\\n",
       "0                ReLU                       ReLU(x) = max(0, x)   \n",
       "1                tanh     tanh(x) = (e^x - e^-x) / (e^x + e^-x)   \n",
       "2             Softmax  Softmax(x) = e^x_i / sum(e^x) for i in x   \n",
       "\n",
       "                                                                                       Characteristics  \\\n",
       "0  Piecewise linear function that outputs the input directly if it's positive, otherwise outputs zero.   \n",
       "1                  Scaled and shifted version of the sigmoid function. Its output ranges from -1 to 1.   \n",
       "2                                         Converts a vector of values into a probability distribution.   \n",
       "\n",
       "                                                                                         Usage  \\\n",
       "0                                             Widely used in hidden layers of neural networks.   \n",
       "1           Commonly used in hidden layers, though less frequent than ReLU in modern networks.   \n",
       "2  Typically used in the output layer of a neural network for multiclass classification tasks.   \n",
       "\n",
       "                                                                                                             Benefits  \\\n",
       "0         Helps alleviate the vanishing gradient problem. Computationally efficient and allows for quick convergence.   \n",
       "1   Can model complex relationships due to its non-linear shape. Useful when data needs to be normalized around zero.   \n",
       "2  Provides a clear probabilistic framework for multiclass classification. Each output class's probability sums to 1.   \n",
       "\n",
       "                                                                                                                Drawbacks  \n",
       "0                           ReLU units can be fragile during training and can 'die' if large gradients flow through them.  \n",
       "1  Similar to sigmoid, it suffers from the vanishing gradient problem, which can make it less effective in deep networks.  \n",
       "2                     Not suitable for non-classification tasks. The exponentiation can cause numerical stability issues.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data_activation_functions = {\n",
    "    \"Activation Function\": [\"ReLU\", \"tanh\", \"Softmax\"],\n",
    "    \"Formula\": [\n",
    "        \"ReLU(x) = max(0, x)\",\n",
    "        \"tanh(x) = (e^x - e^-x) / (e^x + e^-x)\",\n",
    "        \"Softmax(x) = e^x_i / sum(e^x) for i in x\"\n",
    "    ],\n",
    "    \"Characteristics\": [\n",
    "        \"Piecewise linear function that outputs the input directly if it's positive, otherwise outputs zero.\",\n",
    "        \"Scaled and shifted version of the sigmoid function. Its output ranges from -1 to 1.\",\n",
    "        \"Converts a vector of values into a probability distribution.\"\n",
    "    ],\n",
    "    \"Usage\": [\n",
    "        \"Widely used in hidden layers of neural networks.\",\n",
    "        \"Commonly used in hidden layers, though less frequent than ReLU in modern networks.\",\n",
    "        \"Typically used in the output layer of a neural network for multiclass classification tasks.\"\n",
    "    ],\n",
    "    \"Benefits\": [\n",
    "        \"Helps alleviate the vanishing gradient problem. Computationally efficient and allows for quick convergence.\",\n",
    "        \"Can model complex relationships due to its non-linear shape. Useful when data needs to be normalized around zero.\",\n",
    "        \"Provides a clear probabilistic framework for multiclass classification. Each output class's probability sums to 1.\"\n",
    "    ],\n",
    "    \"Drawbacks\": [\n",
    "        \"ReLU units can be fragile during training and can 'die' if large gradients flow through them.\",\n",
    "        \"Similar to sigmoid, it suffers from the vanishing gradient problem, which can make it less effective in deep networks.\",\n",
    "        \"Not suitable for non-classification tasks. The exponentiation can cause numerical stability issues.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_activation_functions = pd.DataFrame(data_activation_functions)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df_activation_functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021e1369951643f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# How weights are modified in Deep Neural Networks\n",
    "\n",
    "Weights play a crucial role in deep neural networks by determining the strength of connections between neurons across layers. Understanding their initialization, role in feed-forward propagation, and optimization through backpropagation is key to grasping how neural networks learn.\n",
    "\n",
    "## Initialization of Weights\n",
    "\n",
    "Weights are initially set to small random values to break symmetry. This ensures that neurons learn different patterns during training. A common initialization strategy is Xavier (Glorot) initialization, defined for a layer \\(l\\) as:\n",
    "\n",
    "$$\n",
    "W^{[l]} \\sim \\text{Uniform}\\left(-\\frac{1}{\\sqrt{n^{[l-1]}}}, \\frac{1}{\\sqrt{n^{[l-1]}}}\\right)\n",
    "$$\n",
    "\n",
    "where $(n^{[l-1]})$ is the number of units in the previous layer. This initialization keeps the variance in activations roughly the same across layers.\n",
    "\n",
    "## Feed-Forward Propagation\n",
    "\n",
    "In the feed-forward phase, inputs are passed through the network to generate an output. For each layer $(l)$, the pre-activation $(z^{[l]})$ and activation $(a^{[l]})$ are computed as:\n",
    "\n",
    "$$\n",
    "z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "$$\n",
    "\n",
    "where $(W^{[l]})$ and $(b^{[l]})$ are the weights and biases for layer $(l)$, $(a^{[l-1]})$ is the activation from the previous layer, and $(g^{[l]})$ is the activation function.\n",
    "\n",
    "## Backpropagation and Optimization\n",
    "\n",
    "Backpropagation is the process used to compute the gradient of the loss function with respect to each weight in the network, which is then used to update the weights and minimize the loss. The update rule for a weight $(W^{[l]})$ using gradient descent is:\n",
    "\n",
    "$$\n",
    "W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $(\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}})$ is the gradient of the loss $\\mathcal{L}$ with respect to $(W^{[l]})$.\n",
    "\n",
    "This process iteratively adjusts the weights to reduce the error between the predicted outputs and the true values, optimizing the network's performance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The initialization, propagation, and optimization of weights are fundamental to the functioning of deep neural networks. Properly initialized weights enable efficient learning, feed-forward propagation leverages these weights to make predictions, and backpropagation optimizes them based on the prediction error, facilitating the network's ability to learn from data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68ddadfee55e7c0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T00:16:48.174592Z",
     "start_time": "2024-02-04T00:16:48.168112Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce0ccf688a42a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2dcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nat_data3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
